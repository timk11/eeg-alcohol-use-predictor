{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook goes through a similar process as the previous one to train models on the UCI dataset, this time with deep learning models. Required libraries and the dataset are first imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.constraints import max_norm\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>Fp1a delta</th>\n",
       "      <th>Fp1a theta</th>\n",
       "      <th>Fp1a alpha</th>\n",
       "      <th>Fp1a beta</th>\n",
       "      <th>Fp1a gamma</th>\n",
       "      <th>Fp2a delta</th>\n",
       "      <th>Fp2a theta</th>\n",
       "      <th>Fp2a alpha</th>\n",
       "      <th>Fp2a beta</th>\n",
       "      <th>...</th>\n",
       "      <th>P3/P4 theta</th>\n",
       "      <th>P3/P4 alpha</th>\n",
       "      <th>P3/P4 beta</th>\n",
       "      <th>P3/P4 gamma</th>\n",
       "      <th>O1/O2 delta</th>\n",
       "      <th>O1/O2 theta</th>\n",
       "      <th>O1/O2 alpha</th>\n",
       "      <th>O1/O2 beta</th>\n",
       "      <th>O1/O2 gamma</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>co3a0000458</td>\n",
       "      <td>5.034276</td>\n",
       "      <td>1.297364</td>\n",
       "      <td>2.776220</td>\n",
       "      <td>7.503791</td>\n",
       "      <td>1.389794</td>\n",
       "      <td>6.732382</td>\n",
       "      <td>1.791624</td>\n",
       "      <td>3.072971</td>\n",
       "      <td>8.030404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.502656</td>\n",
       "      <td>0.517526</td>\n",
       "      <td>0.541235</td>\n",
       "      <td>0.461837</td>\n",
       "      <td>0.480084</td>\n",
       "      <td>0.494039</td>\n",
       "      <td>0.503561</td>\n",
       "      <td>0.577265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>co3a0000459</td>\n",
       "      <td>4.925952</td>\n",
       "      <td>1.284041</td>\n",
       "      <td>3.818269</td>\n",
       "      <td>4.602541</td>\n",
       "      <td>1.256179</td>\n",
       "      <td>4.730081</td>\n",
       "      <td>1.297648</td>\n",
       "      <td>3.812826</td>\n",
       "      <td>8.845018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476234</td>\n",
       "      <td>0.598763</td>\n",
       "      <td>0.568120</td>\n",
       "      <td>0.557745</td>\n",
       "      <td>0.508269</td>\n",
       "      <td>0.549875</td>\n",
       "      <td>0.526056</td>\n",
       "      <td>0.553384</td>\n",
       "      <td>0.539971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>co3a0000460</td>\n",
       "      <td>5.413555</td>\n",
       "      <td>2.577622</td>\n",
       "      <td>2.853991</td>\n",
       "      <td>3.192140</td>\n",
       "      <td>0.737959</td>\n",
       "      <td>5.674537</td>\n",
       "      <td>2.723768</td>\n",
       "      <td>2.885980</td>\n",
       "      <td>3.208463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545288</td>\n",
       "      <td>0.595220</td>\n",
       "      <td>0.539650</td>\n",
       "      <td>0.551718</td>\n",
       "      <td>0.521833</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.516360</td>\n",
       "      <td>0.473023</td>\n",
       "      <td>0.439638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>co3a0000461</td>\n",
       "      <td>7.399629</td>\n",
       "      <td>1.507018</td>\n",
       "      <td>1.856139</td>\n",
       "      <td>2.528717</td>\n",
       "      <td>0.567038</td>\n",
       "      <td>5.505387</td>\n",
       "      <td>1.312715</td>\n",
       "      <td>2.250694</td>\n",
       "      <td>2.524480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605074</td>\n",
       "      <td>0.520692</td>\n",
       "      <td>0.566241</td>\n",
       "      <td>0.594854</td>\n",
       "      <td>0.525616</td>\n",
       "      <td>0.526378</td>\n",
       "      <td>0.529584</td>\n",
       "      <td>0.537405</td>\n",
       "      <td>0.588164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>co3c0000402</td>\n",
       "      <td>5.284542</td>\n",
       "      <td>1.946691</td>\n",
       "      <td>1.135617</td>\n",
       "      <td>2.015422</td>\n",
       "      <td>0.492179</td>\n",
       "      <td>5.092513</td>\n",
       "      <td>1.986647</td>\n",
       "      <td>1.337200</td>\n",
       "      <td>2.166617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497312</td>\n",
       "      <td>0.492201</td>\n",
       "      <td>0.492750</td>\n",
       "      <td>0.493309</td>\n",
       "      <td>0.498716</td>\n",
       "      <td>0.516931</td>\n",
       "      <td>0.544294</td>\n",
       "      <td>0.517307</td>\n",
       "      <td>0.497229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 152 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject  Fp1a delta  Fp1a theta  Fp1a alpha  Fp1a beta  Fp1a gamma  \\\n",
       "117  co3a0000458    5.034276    1.297364    2.776220   7.503791    1.389794   \n",
       "118  co3a0000459    4.925952    1.284041    3.818269   4.602541    1.256179   \n",
       "119  co3a0000460    5.413555    2.577622    2.853991   3.192140    0.737959   \n",
       "120  co3a0000461    7.399629    1.507018    1.856139   2.528717    0.567038   \n",
       "121  co3c0000402    5.284542    1.946691    1.135617   2.015422    0.492179   \n",
       "\n",
       "     Fp2a delta  Fp2a theta  Fp2a alpha  Fp2a beta  ...  P3/P4 theta  \\\n",
       "117    6.732382    1.791624    3.072971   8.030404  ...     0.554300   \n",
       "118    4.730081    1.297648    3.812826   8.845018  ...     0.476234   \n",
       "119    5.674537    2.723768    2.885980   3.208463  ...     0.545288   \n",
       "120    5.505387    1.312715    2.250694   2.524480  ...     0.605074   \n",
       "121    5.092513    1.986647    1.337200   2.166617  ...     0.497312   \n",
       "\n",
       "     P3/P4 alpha  P3/P4 beta  P3/P4 gamma  O1/O2 delta  O1/O2 theta  \\\n",
       "117     0.502656    0.517526     0.541235     0.461837     0.480084   \n",
       "118     0.598763    0.568120     0.557745     0.508269     0.549875   \n",
       "119     0.595220    0.539650     0.551718     0.521833     0.565657   \n",
       "120     0.520692    0.566241     0.594854     0.525616     0.526378   \n",
       "121     0.492201    0.492750     0.493309     0.498716     0.516931   \n",
       "\n",
       "     O1/O2 alpha  O1/O2 beta  O1/O2 gamma  status  \n",
       "117     0.494039    0.503561     0.577265       1  \n",
       "118     0.526056    0.553384     0.539971       1  \n",
       "119     0.516360    0.473023     0.439638       1  \n",
       "120     0.529584    0.537405     0.588164       1  \n",
       "121     0.544294    0.517307     0.497229       0  \n",
       "\n",
       "[5 rows x 152 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('EEG_UCI_dataset_powers.csv')\n",
    "df = df.rename(columns={'Unnamed: 0': 'subject'})\n",
    "df['status'] = (df['subject'].str.slice(start=3, stop=4) == \"a\").astype(int)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcoholic subjects: 77\n",
      "Control subjects: 45\n",
      "Proportion of alcoholic subjects: 0.631\n"
     ]
    }
   ],
   "source": [
    "print(\"Alcoholic subjects:\", len(df.status.loc[df.status ==1]))\n",
    "print(\"Control subjects:\", len(df.status.loc[df.status == 0]))\n",
    "print(\"Proportion of alcoholic subjects:\", round(len(df.status.loc[df.status == 1]) / (len(df.status.loc[df.status == 1]) + len(df.status.loc[df.status == 0])), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset is split into training and test sets, then the KerasClassifier module is used in order to set up a pipeline for a grid search across hyperparameters using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 31 91 31\n"
     ]
    }
   ],
   "source": [
    "y = df.status\n",
    "X = df.drop(['subject', 'status'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234, stratify=df.status)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons1=100, neurons2=30, dropout1=0, dropout2=0, constraint=0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=150, activation='relu', kernel_constraint=max_norm(constraint)))\n",
    "    model.add(Dropout(dropout1))\n",
    "    model.add(Dense(neurons2, activation='relu', kernel_constraint=max_norm(constraint)))\n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('scaler', StandardScaler()), ('nn', KerasClassifier(build_fn=create_model, verbose=0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A number of models are then created using GridSearchCV or RandomizedSearchCV and tested on the test set. The choice of hyperparameters is then refined each time and the most favourable model is saved. (Note that this process is quite time-consuming and was therefore run over a number of sessions, as is apparent from the output numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 375 candidates, totalling 1125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1125 out of 1125 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('nn',\n",
       "                                        <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000015DB3E229C8>)],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'nn__batch_size': [10, 20, 40],\n",
       "                         'nn__epochs': [10, 20, 40, 70, 100],\n",
       "                         'nn__neurons1': [50, 100, 150, 200, 250],\n",
       "                         'nn__neurons2': [10, 20, 30, 40, 50]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 1\n",
    "np.random.seed(12)\n",
    "batch_size = [10, 20, 40]\n",
    "epochs = [10, 20, 40, 70, 100]\n",
    "neurons1 = [50, 100, 150, 200, 250]\n",
    "neurons2 = [10, 20, 30, 40, 50]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2)\n",
    "clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591397960980734 {'nn__batch_size': 40, 'nn__epochs': 10, 'nn__neurons1': 50, 'nn__neurons2': 50}\n",
      "[[ 8  3]\n",
      " [ 4 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        11\n",
      "           1       0.84      0.80      0.82        20\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.75      0.76      0.76        31\n",
      "weighted avg       0.78      0.77      0.78        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1620 candidates, totalling 4860 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.4min\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=6)]: Done 4860 out of 4860 | elapsed: 30.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('nn',\n",
       "                                        <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000015DB3E229C8>)],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=6,\n",
       "             param_grid={'nn__batch_size': [10, 20, 40],\n",
       "                         'nn__dropout1': [0, 0.1, 0.2],\n",
       "                         'nn__dropout2': [0, 0.1, 0.2],\n",
       "                         'nn__epochs': [10, 20, 40],\n",
       "                         'nn__neurons1': [50, 100, 150, 200, 250],\n",
       "                         'nn__neurons2': [30, 40, 50, 60]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 2\n",
    "np.random.seed(12)\n",
    "batch_size = [10, 20, 40]\n",
    "epochs = [10, 20, 40]\n",
    "neurons1 = [50, 100, 150, 200, 250]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2]\n",
    "dropout2 = [0, 0.1, 0.2]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2)\n",
    "clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=6, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7802867293357849 {'nn__batch_size': 20, 'nn__dropout1': 0, 'nn__dropout2': 0.2, 'nn__epochs': 10, 'nn__neurons1': 50, 'nn__neurons2': 50}\n",
      "[[ 8  3]\n",
      " [ 7 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.73      0.62        11\n",
      "           1       0.81      0.65      0.72        20\n",
      "\n",
      "    accuracy                           0.68        31\n",
      "   macro avg       0.67      0.69      0.67        31\n",
      "weighted avg       0.71      0.68      0.68        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('scaler',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('nn',\n",
       "                                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F116E1F108>)],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=1000, n_jobs=3,\n",
       "                   param_distributions={'nn__batch_size': [10, 20, 30, 40],\n",
       "                                        'nn__constraint': [0, 1, 2, 3, 4, 5],\n",
       "                                        'nn__dropout1': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__dropout2': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__epochs': [8, 10, 12, 14, 16, 18,\n",
       "                                                       20],\n",
       "                                        'nn__neurons1': [50, 75, 100, 125, 150,\n",
       "                                                         175, 200],\n",
       "                                        'nn__neurons2': [30, 40, 50, 60]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 3\n",
    "np.random.seed(12)\n",
    "batch_size = [10, 20, 30, 40]\n",
    "epochs = [8, 10, 12, 14, 16, 18, 20]\n",
    "neurons1 = [50, 75, 100, 125, 150, 175, 200]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\timk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "0.7806451519330343 {'nn__neurons2': 30, 'nn__neurons1': 100, 'nn__epochs': 8, 'nn__dropout2': 0.4, 'nn__dropout1': 0.4, 'nn__constraint': 4, 'nn__batch_size': 40}\n",
      "[[ 8  3]\n",
      " [ 5 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.73      0.67        11\n",
      "           1       0.83      0.75      0.79        20\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.72      0.74      0.73        31\n",
      "weighted avg       0.76      0.74      0.75        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.6s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('scaler',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('nn',\n",
       "                                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F116E1F108>)],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=1000, n_jobs=3,\n",
       "                   param_distributions={'nn__batch_size': [30, 35, 40, 45, 50],\n",
       "                                        'nn__constraint': [0, 1, 2, 3, 4, 5],\n",
       "                                        'nn__dropout1': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__dropout2': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__epochs': [6, 7, 8, 9, 10, 11, 12],\n",
       "                                        'nn__neurons1': [25, 37, 50, 75, 100],\n",
       "                                        'nn__neurons2': [30, 40, 50, 60]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 4\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45, 50]\n",
    "epochs = [6, 7, 8, 9, 10, 11, 12]\n",
    "neurons1 = [25, 37, 50, 75, 100]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000, random_state=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8139784932136536 {'nn__neurons2': 50, 'nn__neurons1': 75, 'nn__epochs': 11, 'nn__dropout2': 0.4, 'nn__dropout1': 0.5, 'nn__constraint': 3, 'nn__batch_size': 45}\n",
      "[[ 8  3]\n",
      " [ 4 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        11\n",
      "           1       0.84      0.80      0.82        20\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.75      0.76      0.76        31\n",
      "weighted avg       0.78      0.77      0.78        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_dl_model = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   33.6s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=6)]: Done 1944 out of 1944 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('nn',\n",
       "                                        <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001F116E1F108>)],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=6,\n",
       "             param_grid={'nn__batch_size': [40, 45, 50],\n",
       "                         'nn__constraint': [3, 4], 'nn__dropout1': [0.4, 0.5],\n",
       "                         'nn__dropout2': [0.4, 0.5], 'nn__epochs': [10, 11, 12],\n",
       "                         'nn__neurons1': [67, 75, 83],\n",
       "                         'nn__neurons2': [45, 50, 55]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 5\n",
    "np.random.seed(12)\n",
    "batch_size = [40, 45, 50]\n",
    "epochs = [10, 11, 12]\n",
    "neurons1 = [67, 75, 83]\n",
    "neurons2 = [45, 50, 55]\n",
    "dropout1 = [0.4, 0.5]\n",
    "dropout2 = [0.4, 0.5]\n",
    "constraint = [3, 4]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=6, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8035842378934225 {'nn__batch_size': 45, 'nn__constraint': 3, 'nn__dropout1': 0.5, 'nn__dropout2': 0.4, 'nn__epochs': 10, 'nn__neurons1': 75, 'nn__neurons2': 55}\n",
      "[[ 6  5]\n",
      " [ 3 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.55      0.60        11\n",
      "           1       0.77      0.85      0.81        20\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.72      0.70      0.70        31\n",
      "weighted avg       0.74      0.74      0.74        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model 1 and model 4 have produced the best results, so as model 4 has already been stored in a variable this will be saved for later use.\n",
    "> \n",
    "> The entire model is not suitable for saving in a single file with either the Pickle or Joblib libraries, so it is split into two parts and saved as two files. The process for saving it this way and subsequently retrieving it is modified from https://prodevsblog.com/questions/125081/how-to-save-a-scikit-learn-pipline-with-keras-regressor-inside-to-disk/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eeg_dl_model.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_dl_model.named_steps['nn'].model.save('eeg_dl_model.h5')\n",
    "fitted_dl_model.named_steps['nn'].model = None\n",
    "joblib.dump(fitted_dl_model, 'eeg_dl_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nested cross validation model**\n",
    ">\n",
    "> Nested cross validation is applied, as per the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   15.1s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\timk\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "\n",
      " 0.7154121994972229 {'nn__neurons2': 50, 'nn__neurons1': 25, 'nn__epochs': 7, 'nn__dropout2': 0, 'nn__dropout1': 0.5, 'nn__constraint': 5, 'nn__batch_size': 45}\n",
      "[[ 4  7]\n",
      " [ 2 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.36      0.47        11\n",
      "           1       0.72      0.90      0.80        20\n",
      "\n",
      "    accuracy                           0.71        31\n",
      "   macro avg       0.69      0.63      0.64        31\n",
      "weighted avg       0.70      0.71      0.68        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.3s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7448028723398844 {'nn__neurons2': 40, 'nn__neurons1': 50, 'nn__epochs': 6, 'nn__dropout2': 0.3, 'nn__dropout1': 0.5, 'nn__constraint': 2, 'nn__batch_size': 45}\n",
      "[[ 8  4]\n",
      " [ 1 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.67      0.76        12\n",
      "           1       0.82      0.95      0.88        19\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.85      0.81      0.82        31\n",
      "weighted avg       0.85      0.84      0.83        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7279569904009501 {'nn__neurons2': 30, 'nn__neurons1': 25, 'nn__epochs': 9, 'nn__dropout2': 0, 'nn__dropout1': 0.1, 'nn__constraint': 4, 'nn__batch_size': 30}\n",
      "[[ 4  7]\n",
      " [ 3 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.36      0.44        11\n",
      "           1       0.70      0.84      0.76        19\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.63      0.60      0.60        30\n",
      "weighted avg       0.65      0.67      0.65        30\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   45.6s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7935483853022257 {'nn__neurons2': 60, 'nn__neurons1': 25, 'nn__epochs': 8, 'nn__dropout2': 0.4, 'nn__dropout1': 0, 'nn__constraint': 1, 'nn__batch_size': 35}\n",
      "[[ 6  5]\n",
      " [ 6 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.52        11\n",
      "           1       0.72      0.68      0.70        19\n",
      "\n",
      "    accuracy                           0.63        30\n",
      "   macro avg       0.61      0.61      0.61        30\n",
      "weighted avg       0.64      0.63      0.64        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.7856640612738174\n",
      "  sensitivity : 0.8434210526315788\n",
      "  specificity : 0.48484848484848486\n",
      "  PPV : 0.739014053579271\n",
      "  NPV : 0.6567460317460316\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 1\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45, 50]\n",
    "epochs = [6, 7, 8, 9, 10, 11, 12]\n",
    "neurons1 = [25, 37, 50, 75, 100]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.0s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F9A93A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7580645084381104 {'nn__neurons2': 50, 'nn__neurons1': 35, 'nn__epochs': 8, 'nn__dropout2': 0.3, 'nn__dropout1': 0.3, 'nn__constraint': 2, 'nn__batch_size': 45}\n",
      "[[ 7  4]\n",
      " [ 7 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.64      0.56        11\n",
      "           1       0.76      0.65      0.70        20\n",
      "\n",
      "    accuracy                           0.65        31\n",
      "   macro avg       0.63      0.64      0.63        31\n",
      "weighted avg       0.67      0.65      0.65        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.7s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F60A798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7240143418312073 {'nn__neurons2': 40, 'nn__neurons1': 35, 'nn__epochs': 9, 'nn__dropout2': 0.3, 'nn__dropout1': 0.4, 'nn__constraint': 4, 'nn__batch_size': 40}\n",
      "[[ 6  6]\n",
      " [ 0 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        12\n",
      "           1       0.76      1.00      0.86        19\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.88      0.75      0.77        31\n",
      "weighted avg       0.85      0.81      0.79        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7C411678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7390681107838949 {'nn__neurons2': 40, 'nn__neurons1': 40, 'nn__epochs': 8, 'nn__dropout2': 0.5, 'nn__dropout1': 0.3, 'nn__constraint': 4, 'nn__batch_size': 45}\n",
      "[[ 6  5]\n",
      " [ 3 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.55      0.60        11\n",
      "           1       0.76      0.84      0.80        19\n",
      "\n",
      "    accuracy                           0.73        30\n",
      "   macro avg       0.71      0.69      0.70        30\n",
      "weighted avg       0.73      0.73      0.73        30\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F8870D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.8146953384081522 {'nn__neurons2': 30, 'nn__neurons1': 35, 'nn__epochs': 6, 'nn__dropout2': 0, 'nn__dropout1': 0.1, 'nn__constraint': 2, 'nn__batch_size': 45}\n",
      "[[ 6  5]\n",
      " [ 4 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.55      0.57        11\n",
      "           1       0.75      0.79      0.77        19\n",
      "\n",
      "    accuracy                           0.70        30\n",
      "   macro avg       0.68      0.67      0.67        30\n",
      "weighted avg       0.70      0.70      0.70        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.783892458892459\n",
      "  sensitivity : 0.8203947368421052\n",
      "  specificity : 0.5568181818181818\n",
      "  PPV : 0.7591526610644257\n",
      "  NPV : 0.6916666666666667\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 2\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45]\n",
    "epochs = [6, 7, 8, 9]\n",
    "neurons1 = [25, 30, 35, 40, 45, 50]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   32.3s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=6)]: Done 5184 out of 5184 | elapsed: 14.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F728438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7684587637583414 {'nn__batch_size': 45, 'nn__constraint': 4, 'nn__dropout1': 0.4, 'nn__dropout2': 0.3, 'nn__epochs': 6, 'nn__neurons1': 40, 'nn__neurons2': 50}\n",
      "[[ 6  5]\n",
      " [ 5 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.55      0.55        11\n",
      "           1       0.75      0.75      0.75        20\n",
      "\n",
      "    accuracy                           0.68        31\n",
      "   macro avg       0.65      0.65      0.65        31\n",
      "weighted avg       0.68      0.68      0.68        31\n",
      "\n",
      "Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    6.2s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=6)]: Done 5184 out of 5184 | elapsed: 14.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7C411B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.8333333333333334 {'nn__batch_size': 45, 'nn__constraint': 3, 'nn__dropout1': 0.3, 'nn__dropout2': 0, 'nn__epochs': 7, 'nn__neurons1': 35, 'nn__neurons2': 30}\n",
      "[[ 6  6]\n",
      " [ 1 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.50      0.63        12\n",
      "           1       0.75      0.95      0.84        19\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.80      0.72      0.73        31\n",
      "weighted avg       0.79      0.77      0.76        31\n",
      "\n",
      "Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   34.4s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=6)]: Done 5184 out of 5184 | elapsed: 14.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F60A3A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.792831540107727 {'nn__batch_size': 45, 'nn__constraint': 4, 'nn__dropout1': 0.2, 'nn__dropout2': 0.3, 'nn__epochs': 6, 'nn__neurons1': 35, 'nn__neurons2': 30}\n",
      "[[ 6  5]\n",
      " [ 9 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.55      0.46        11\n",
      "           1       0.67      0.53      0.59        19\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.53      0.54      0.52        30\n",
      "weighted avg       0.57      0.53      0.54        30\n",
      "\n",
      "Fitting 3 folds for each of 1728 candidates, totalling 5184 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    5.9s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done 788 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=6)]: Done 1238 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=6)]: Done 1788 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=6)]: Done 2438 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=6)]: Done 3188 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=6)]: Done 4038 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=6)]: Done 4988 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=6)]: Done 5184 out of 5184 | elapsed: 14.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7F60A678> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.8043010830879211 {'nn__batch_size': 40, 'nn__constraint': 4, 'nn__dropout1': 0.4, 'nn__dropout2': 0.5, 'nn__epochs': 8, 'nn__neurons1': 40, 'nn__neurons2': 40}\n",
      "[[ 6  5]\n",
      " [ 8 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.55      0.48        11\n",
      "           1       0.69      0.58      0.63        19\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.56      0.56      0.55        30\n",
      "weighted avg       0.59      0.57      0.57        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.7010040062536643\n",
      "  sensitivity : 0.700657894736842\n",
      "  specificity : 0.5340909090909091\n",
      "  PPV : 0.7135416666666666\n",
      "  NPV : 0.5577922077922077\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 3\n",
    "np.random.seed(12)\n",
    "batch_size = [40, 45]\n",
    "epochs = [6, 7, 8, 9]\n",
    "neurons1 = [35, 40]\n",
    "neurons2 = [30, 40, 50]\n",
    "dropout1 = [0.1, 0.2, 0.3, 0.4]\n",
    "dropout2 = [0, 0.3, 0.5]\n",
    "constraint = [2, 3, 4]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=6, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Process 2 has produced the most helpful set of results, in particular the highest positive and negative predictive values (class 1 and 0 precision respectively), so this process is repeated on the full dataset using 3-fold cross validation. The resulting model is then saved as two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.8s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   45.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('scaler',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('nn',\n",
       "                                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001FD7C2C1F08>)],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=1000, n_jobs=3,\n",
       "                   param_distributions={'nn__batch_size': [30, 35, 40, 45],\n",
       "                                        'nn__constraint': [0, 1, 2, 3, 4, 5],\n",
       "                                        'nn__dropout1': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__dropout2': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__epochs': [6, 7, 8, 9],\n",
       "                                        'nn__neurons1': [25, 30, 35, 40, 45,\n",
       "                                                         50],\n",
       "                                        'nn__neurons2': [30, 40, 50, 60]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45]\n",
    "epochs = [6, 7, 8, 9]\n",
    "neurons1 = [25, 30, 35, 40, 45, 50]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000, random_state=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7697154482205709 {'nn__neurons2': 30, 'nn__neurons1': 50, 'nn__epochs': 6, 'nn__dropout2': 0.5, 'nn__dropout1': 0, 'nn__constraint': 3, 'nn__batch_size': 45}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_score_, clf.best_params_)\n",
    "fitted_dl_model_ncv = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eeg_dl_model_ncv.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_dl_model_ncv.named_steps['nn'].model.save('eeg_dl_model_ncv.h5')\n",
    "fitted_dl_model_ncv.named_steps['nn'].model = None\n",
    "joblib.dump(fitted_dl_model_ncv, 'eeg_dl_model_ncv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WITHOUT ABSOLUTE POWERS**\n",
    ">\n",
    "> A similar set of models is then produced using a version of the dataset without absolute powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.status\n",
    "X = df.drop(['subject', 'Fp1a delta', 'Fp1a theta', 'Fp1a alpha', 'Fp1a beta',\n",
    "       'Fp1a gamma', 'Fp2a delta', 'Fp2a theta', 'Fp2a alpha',\n",
    "       'Fp2a beta', 'Fp2a gamma', 'F3a delta', 'F3a theta', 'F3a alpha',\n",
    "       'F3a beta', 'F3a gamma', 'F4a delta', 'F4a theta', 'F4a alpha',\n",
    "       'F4a beta', 'F4a gamma', 'F7a delta', 'F7a theta', 'F7a alpha',\n",
    "       'F7a beta', 'F7a gamma', 'F8a delta', 'F8a theta', 'F8a alpha',\n",
    "       'F8a beta', 'F8a gamma', 'C3a delta', 'C3a theta', 'C3a alpha',\n",
    "       'C3a beta', 'C3a gamma', 'C4a delta', 'C4a theta', 'C4a alpha',\n",
    "       'C4a beta', 'C4a gamma', 'P3a delta', 'P3a theta', 'P3a alpha',\n",
    "       'P3a beta', 'P3a gamma', 'P4a delta', 'P4a theta', 'P4a alpha',\n",
    "       'P4a beta', 'P4a gamma', 'O1a delta', 'O1a theta', 'O1a alpha',\n",
    "       'O1a beta', 'O1a gamma', 'O2a delta', 'O2a theta', 'O2a alpha',\n",
    "       'O2a beta', 'O2a gamma', 'status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1234, stratify=df.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons1=100, neurons2=30, dropout1=0, dropout2=0, constraint=0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=90, activation='relu', kernel_constraint=max_norm(constraint)))\n",
    "    model.add(Dropout(dropout1))\n",
    "    model.add(Dense(neurons2, activation='relu', kernel_constraint=max_norm(constraint)))\n",
    "    model.add(Dropout(dropout2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('nn', KerasClassifier(build_fn=create_model, verbose=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=3)]: Done 3000 out of 3000 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('scaler',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('nn',\n",
       "                                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001FD7F81BA08>)],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=1000, n_jobs=3,\n",
       "                   param_distributions={'nn__batch_size': [30, 35, 40, 45, 50],\n",
       "                                        'nn__constraint': [0, 1, 2, 3, 4, 5],\n",
       "                                        'nn__dropout1': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__dropout2': [0, 0.1, 0.2, 0.3, 0.4,\n",
       "                                                         0.5],\n",
       "                                        'nn__epochs': [6, 7, 8, 9, 10, 11, 12],\n",
       "                                        'nn__neurons1': [25, 37, 50, 75, 100],\n",
       "                                        'nn__neurons2': [30, 40, 50, 60]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 1\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45, 50]\n",
    "epochs = [6, 7, 8, 9, 10, 11, 12]\n",
    "neurons1 = [25, 37, 50, 75, 100]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7C74B558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.7265233000119528 {'nn__neurons2': 50, 'nn__neurons1': 37, 'nn__epochs': 9, 'nn__dropout2': 0.1, 'nn__dropout1': 0.5, 'nn__constraint': 5, 'nn__batch_size': 30}\n",
      "[[ 5  6]\n",
      " [ 3 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53        11\n",
      "           1       0.74      0.85      0.79        20\n",
      "\n",
      "    accuracy                           0.71        31\n",
      "   macro avg       0.68      0.65      0.66        31\n",
      "weighted avg       0.70      0.71      0.70        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_dl_model_no_ap = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 972 candidates, totalling 2916 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.5s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=3)]: Done 2916 out of 2916 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('nn',\n",
       "                                        <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001FD7F81BA08>)],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=3,\n",
       "             param_grid={'nn__batch_size': [25, 30, 35],\n",
       "                         'nn__constraint': [4, 5], 'nn__dropout1': [0.4, 0.5],\n",
       "                         'nn__dropout2': [0, 0.1, 0.2],\n",
       "                         'nn__epochs': [8, 9, 10], 'nn__neurons1': [31, 37, 43],\n",
       "                         'nn__neurons2': [45, 50, 55]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 2\n",
    "np.random.seed(12)\n",
    "batch_size = [25, 30, 35]\n",
    "epochs = [8, 9, 10]\n",
    "neurons1 = [31, 37, 43]\n",
    "neurons2 = [45, 50, 55]\n",
    "dropout1 = [0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2]\n",
    "constraint = [4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=3, verbose=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7E22C798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0.7591397762298584 {'nn__batch_size': 35, 'nn__constraint': 4, 'nn__dropout1': 0.4, 'nn__dropout2': 0.1, 'nn__epochs': 8, 'nn__neurons1': 37, 'nn__neurons2': 45}\n",
      "[[ 6  5]\n",
      " [ 6 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.52        11\n",
      "           1       0.74      0.70      0.72        20\n",
      "\n",
      "    accuracy                           0.65        31\n",
      "   macro avg       0.62      0.62      0.62        31\n",
      "weighted avg       0.65      0.65      0.65        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "print(clf.best_score_, clf.best_params_)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model 1 has produced the more clinically useful set of results so this one is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eeg_dl_model_no_ap.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_dl_model_no_ap.named_steps['nn'].model.save('eeg_dl_model_no_ap.h5')\n",
    "fitted_dl_model_no_ap.named_steps['nn'].model = None\n",
    "joblib.dump(fitted_dl_model_no_ap, 'eeg_dl_model_no_ap.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nested cross validation model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   34.5s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=5)]: Done 3000 out of 3000 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD09249558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.6594982147216797 {'nn__neurons2': 30, 'nn__neurons1': 25, 'nn__epochs': 6, 'nn__dropout2': 0.2, 'nn__dropout1': 0.3, 'nn__constraint': 3, 'nn__batch_size': 35}\n",
      "[[ 4  7]\n",
      " [ 6 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.36      0.38        11\n",
      "           1       0.67      0.70      0.68        20\n",
      "\n",
      "    accuracy                           0.58        31\n",
      "   macro avg       0.53      0.53      0.53        31\n",
      "weighted avg       0.57      0.58      0.58        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    7.8s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   34.6s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=5)]: Done 3000 out of 3000 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD09388C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7347670197486877 {'nn__neurons2': 40, 'nn__neurons1': 25, 'nn__epochs': 7, 'nn__dropout2': 0.2, 'nn__dropout1': 0, 'nn__constraint': 2, 'nn__batch_size': 30}\n",
      "[[ 2 10]\n",
      " [ 0 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.17      0.29        12\n",
      "           1       0.66      1.00      0.79        19\n",
      "\n",
      "    accuracy                           0.68        31\n",
      "   macro avg       0.83      0.58      0.54        31\n",
      "weighted avg       0.79      0.68      0.60        31\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=5)]: Done 3000 out of 3000 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FB213A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.716487447420756 {'nn__neurons2': 30, 'nn__neurons1': 25, 'nn__epochs': 6, 'nn__dropout2': 0.5, 'nn__dropout1': 0, 'nn__constraint': 5, 'nn__batch_size': 30}\n",
      "[[ 4  7]\n",
      " [ 6 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.36      0.38        11\n",
      "           1       0.65      0.68      0.67        19\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.53      0.52      0.52        30\n",
      "weighted avg       0.56      0.57      0.56        30\n",
      "\n",
      "Fitting 3 folds for each of 1000 candidates, totalling 3000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   35.1s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=5)]: Done 3000 out of 3000 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD08FE2048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.6724014480908712 {'nn__neurons2': 30, 'nn__neurons1': 25, 'nn__epochs': 11, 'nn__dropout2': 0.2, 'nn__dropout1': 0.3, 'nn__constraint': 5, 'nn__batch_size': 45}\n",
      "[[ 4  7]\n",
      " [ 9 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.36      0.33        11\n",
      "           1       0.59      0.53      0.56        19\n",
      "\n",
      "    accuracy                           0.47        30\n",
      "   macro avg       0.45      0.44      0.44        30\n",
      "weighted avg       0.49      0.47      0.47        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.6742039295392954\n",
      "  sensitivity : 0.7276315789473684\n",
      "  specificity : 0.3143939393939394\n",
      "  PPV : 0.6400185936443543\n",
      "  NPV : 0.5269230769230768\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 1\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45, 50]\n",
    "epochs = [6, 7, 8, 9, 10, 11, 12]\n",
    "neurons1 = [25, 37, 50, 75, 100]\n",
    "neurons2 = [30, 40, 50, 60]\n",
    "dropout1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "dropout2 = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "constraint = [0, 1, 2, 3, 4, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=5, verbose=1, n_iter=1000, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.1s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1296 out of 1296 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FEC91F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7792114814122518 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.2, 'nn__epochs': 6, 'nn__neurons1': 20, 'nn__neurons2': 40}\n",
      "[[ 8  3]\n",
      " [ 9 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.73      0.57        11\n",
      "           1       0.79      0.55      0.65        20\n",
      "\n",
      "    accuracy                           0.61        31\n",
      "   macro avg       0.63      0.64      0.61        31\n",
      "weighted avg       0.67      0.61      0.62        31\n",
      "\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.4s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1296 out of 1296 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FC075E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.767025093237559 {'nn__batch_size': 37, 'nn__constraint': 5, 'nn__dropout1': 0, 'nn__dropout2': 0.5, 'nn__epochs': 7, 'nn__neurons1': 25, 'nn__neurons2': 20}\n",
      "[[ 2 10]\n",
      " [ 1 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.17      0.27        12\n",
      "           1       0.64      0.95      0.77        19\n",
      "\n",
      "    accuracy                           0.65        31\n",
      "   macro avg       0.65      0.56      0.52        31\n",
      "weighted avg       0.65      0.65      0.57        31\n",
      "\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   44.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1296 out of 1296 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD09303CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7483870983123779 {'nn__batch_size': 37, 'nn__constraint': 2, 'nn__dropout1': 0, 'nn__dropout2': 0.5, 'nn__epochs': 6, 'nn__neurons1': 20, 'nn__neurons2': 20}\n",
      "[[ 2  9]\n",
      " [ 3 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.18      0.25        11\n",
      "           1       0.64      0.84      0.73        19\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.52      0.51      0.49        30\n",
      "weighted avg       0.55      0.60      0.55        30\n",
      "\n",
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.6s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1296 out of 1296 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD08FE2828> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7931899627049764 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 6, 'nn__neurons1': 20, 'nn__neurons2': 30}\n",
      "[[ 5  6]\n",
      " [ 4 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.45      0.50        11\n",
      "           1       0.71      0.79      0.75        19\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.63      0.62      0.62        30\n",
      "weighted avg       0.66      0.67      0.66        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.7225722494026624\n",
      "  sensitivity : 0.7822368421052632\n",
      "  specificity : 0.38257575757575757\n",
      "  PPV : 0.6957142857142857\n",
      "  NPV : 0.5232026143790849\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 2\n",
    "batch_size = [30, 37, 45]\n",
    "epochs = [6, 7]\n",
    "neurons1 = [20, 25]\n",
    "neurons2 = [20, 30, 40]\n",
    "dropout1 = [0, 0.3]\n",
    "dropout2 = [0.2, 0.5]\n",
    "constraint = [2, 3, 5]\n",
    "np.random.seed(12)\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=3, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 105 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.2s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=3)]: Done 315 out of 315 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FA4FD38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7573476632436117 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 7, 'nn__neurons1': 30, 'nn__neurons2': 20}\n",
      "[[11  0]\n",
      " [17  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      1.00      0.56        11\n",
      "           1       1.00      0.15      0.26        20\n",
      "\n",
      "    accuracy                           0.45        31\n",
      "   macro avg       0.70      0.57      0.41        31\n",
      "weighted avg       0.78      0.45      0.37        31\n",
      "\n",
      "Fitting 3 folds for each of 105 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.6s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=3)]: Done 315 out of 315 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FEC9798> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7666666706403097 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 6, 'nn__neurons1': 35, 'nn__neurons2': 40}\n",
      "[[ 6  6]\n",
      " [ 4 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55        12\n",
      "           1       0.71      0.79      0.75        19\n",
      "\n",
      "    accuracy                           0.68        31\n",
      "   macro avg       0.66      0.64      0.65        31\n",
      "weighted avg       0.67      0.68      0.67        31\n",
      "\n",
      "Fitting 3 folds for each of 105 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=3)]: Done 315 out of 315 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD7FC9B318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.6645161310831705 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 7, 'nn__neurons1': 20, 'nn__neurons2': 40}\n",
      "[[ 4  7]\n",
      " [ 5 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.36      0.40        11\n",
      "           1       0.67      0.74      0.70        19\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.56      0.55      0.55        30\n",
      "weighted avg       0.59      0.60      0.59        30\n",
      "\n",
      "Fitting 3 folds for each of 105 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=3)]: Done 315 out of 315 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FD09359708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7172042926152548 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 8, 'nn__neurons1': 25, 'nn__neurons2': 30}\n",
      "[[ 2  9]\n",
      " [ 4 15]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.18      0.24        11\n",
      "           1       0.62      0.79      0.70        19\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.48      0.49      0.47        30\n",
      "weighted avg       0.52      0.57      0.53        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.6021359959555106\n",
      "  sensitivity : 0.6164473684210526\n",
      "  specificity : 0.5113636363636364\n",
      "  PPV : 0.7514880952380952\n",
      "  NPV : 0.44265873015873014\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 3\n",
    "np.random.seed(12)\n",
    "batch_size = [45]\n",
    "epochs = [6, 7, 8, 9, 10, 11, 12]\n",
    "neurons1 = [20, 25, 30, 35, 40]\n",
    "neurons2 = [20, 30, 40]\n",
    "dropout1 = [0.3]\n",
    "dropout2 = [0.5]\n",
    "constraint = [5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=3, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3072 candidates, totalling 9216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.7s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=3)]: Done 3194 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=3)]: Done 4044 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=3)]: Done 4994 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=3)]: Done 6044 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=3)]: Done 7194 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=3)]: Done 8444 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=3)]: Done 9216 out of 9216 | elapsed: 34.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7777777910232544 {'nn__batch_size': 35, 'nn__constraint': 3, 'nn__dropout1': 0.3, 'nn__dropout2': 0.5, 'nn__epochs': 6, 'nn__neurons1': 20, 'nn__neurons2': 30}\n",
      "[[ 2  9]\n",
      " [ 0 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.18      0.31        11\n",
      "           1       0.69      1.00      0.82        20\n",
      "\n",
      "    accuracy                           0.71        31\n",
      "   macro avg       0.84      0.59      0.56        31\n",
      "weighted avg       0.80      0.71      0.64        31\n",
      "\n",
      "Fitting 3 folds for each of 3072 candidates, totalling 9216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   43.3s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=3)]: Done 3194 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=3)]: Done 4044 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=3)]: Done 4994 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=3)]: Done 6044 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=3)]: Done 7194 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=3)]: Done 8444 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=3)]: Done 9216 out of 9216 | elapsed: 34.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.7781361937522888 {'nn__batch_size': 30, 'nn__constraint': 3, 'nn__dropout1': 0, 'nn__dropout2': 0.5, 'nn__epochs': 7, 'nn__neurons1': 25, 'nn__neurons2': 50}\n",
      "[[ 3  9]\n",
      " [ 1 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.25      0.38        12\n",
      "           1       0.67      0.95      0.78        19\n",
      "\n",
      "    accuracy                           0.68        31\n",
      "   macro avg       0.71      0.60      0.58        31\n",
      "weighted avg       0.70      0.68      0.62        31\n",
      "\n",
      "Fitting 3 folds for each of 3072 candidates, totalling 9216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=3)]: Done 3194 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=3)]: Done 4044 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=3)]: Done 4994 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=3)]: Done 6044 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=3)]: Done 7194 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=3)]: Done 8444 tasks      | elapsed: 31.7min\n",
      "[Parallel(n_jobs=3)]: Done 9216 out of 9216 | elapsed: 34.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D33C18A708> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.7720430095990499 {'nn__batch_size': 45, 'nn__constraint': 5, 'nn__dropout1': 0.3, 'nn__dropout2': 0.2, 'nn__epochs': 8, 'nn__neurons1': 20, 'nn__neurons2': 20}\n",
      "[[ 7  4]\n",
      " [15  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.64      0.42        11\n",
      "           1       0.50      0.21      0.30        19\n",
      "\n",
      "    accuracy                           0.37        30\n",
      "   macro avg       0.41      0.42      0.36        30\n",
      "weighted avg       0.43      0.37      0.34        30\n",
      "\n",
      "Fitting 3 folds for each of 3072 candidates, totalling 9216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=3)]: Done 1794 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=3)]: Done 2444 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=3)]: Done 3194 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=3)]: Done 4044 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=3)]: Done 4994 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=3)]: Done 6044 tasks      | elapsed: 22.8min\n",
      "[Parallel(n_jobs=3)]: Done 7194 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=3)]: Done 8444 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=3)]: Done 9216 out of 9216 | elapsed: 34.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001D33AD1E318> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      " 0.782437264919281 {'nn__batch_size': 45, 'nn__constraint': 3, 'nn__dropout1': 0, 'nn__dropout2': 0.5, 'nn__epochs': 9, 'nn__neurons1': 20, 'nn__neurons2': 20}\n",
      "[[ 1 10]\n",
      " [ 3 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.09      0.13        11\n",
      "           1       0.62      0.84      0.71        19\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.43      0.47      0.42        30\n",
      "weighted avg       0.48      0.57      0.50        30\n",
      "\n",
      "\n",
      "Mean scores:\n",
      "  f1 : 0.6515856584179566\n",
      "  sensitivity : 0.75\n",
      "  specificity : 0.28977272727272724\n",
      "  PPV : 0.6179266136162688\n",
      "  NPV : 0.5795454545454546\n"
     ]
    }
   ],
   "source": [
    "# PROCESS 4\n",
    "np.random.seed(12)\n",
    "batch_size = [30, 35, 40, 45]\n",
    "epochs = [6, 7, 8, 9]\n",
    "neurons1 = [20, 25, 30, 35]\n",
    "neurons2 = [20, 30, 40, 50]\n",
    "dropout1 = [0, 0.3]\n",
    "dropout2 = [0.2, 0.5]\n",
    "constraint = [2, 3, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "cv_outer = StratifiedKFold(n_splits=4, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_ix, :], X.iloc[test_ix, :]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "    cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    clf = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=3, n_jobs=3, verbose=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = (clf.predict(X_test) > 0.5).astype(int)\n",
    "    print(\"\\n\", clf.best_score_, clf.best_params_)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    outer_results.append({'f1': f1_score(y_test, y_pred), 'sensitivity': recall_score(y_test, y_pred),\n",
    "                          'specificity': recall_score(y_test, y_pred, pos_label=0), 'PPV': precision_score(y_test, y_pred),\n",
    "                          'NPV': precision_score(y_test, y_pred, pos_label=0)})\n",
    "print(\"\\nMean scores:\")\n",
    "for score in ['f1', 'sensitivity', 'specificity', 'PPV', 'NPV']:\n",
    "    print(\" \", score, \":\", np.array([dict[score] for dict in outer_results]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> None of these results are suggesting a very strong predictor, but process 2 is the best of these so this is applied to the full dataset using 3-fold cross validation and the resulting model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 432 is smaller than n_iter=1000. Running 432 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   12.4s\n",
      "C:\\Users\\timk\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=3)]: Done 1244 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1296 out of 1296 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('scaler',\n",
       "                                              StandardScaler(copy=True,\n",
       "                                                             with_mean=True,\n",
       "                                                             with_std=True)),\n",
       "                                             ('nn',\n",
       "                                              <tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001D3316534C8>)],\n",
       "                                      verbose=False),\n",
       "                   iid='deprecated', n_iter=1000, n_jobs=3,\n",
       "                   param_distributions={'nn__batch_size': [30, 37, 45],\n",
       "                                        'nn__constraint': [2, 3, 5],\n",
       "                                        'nn__dropout1': [0, 0.3],\n",
       "                                        'nn__dropout2': [0.2, 0.5],\n",
       "                                        'nn__epochs': [6, 7],\n",
       "                                        'nn__neurons1': [20, 25],\n",
       "                                        'nn__neurons2': [20, 30, 40]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "batch_size = [30, 37, 45]\n",
    "epochs = [6, 7]\n",
    "neurons1 = [20, 25]\n",
    "neurons2 = [20, 30, 40]\n",
    "dropout1 = [0, 0.3]\n",
    "dropout2 = [0.2, 0.5]\n",
    "constraint = [2, 3, 5]\n",
    "param_grid = dict(nn__batch_size=batch_size, nn__epochs=epochs, nn__neurons1=neurons1, nn__neurons2=neurons2,\n",
    "                 nn__dropout1=dropout1, nn__dropout2=dropout2, nn__constraint=constraint)\n",
    "clf = RandomizedSearchCV(estimator=pipeline, param_distributions=param_grid, cv=3, n_jobs=3, verbose=1, n_iter=1000, random_state=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6959349711736044 {'nn__neurons2': 20, 'nn__neurons1': 20, 'nn__epochs': 6, 'nn__dropout2': 0.5, 'nn__dropout1': 0.3, 'nn__constraint': 5, 'nn__batch_size': 30}\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_score_, clf.best_params_)\n",
    "fitted_dl_model_no_ap_ncv = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eeg_dl_model_no_ap_ncv.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_dl_model_no_ap_ncv.named_steps['nn'].model.save('eeg_dl_model_no_ap_ncv.h5')\n",
    "fitted_dl_model_no_ap_ncv.named_steps['nn'].model = None\n",
    "joblib.dump(fitted_dl_model_no_ap_ncv, 'eeg_dl_model_no_ap_ncv.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
